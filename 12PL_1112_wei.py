#20250818è£œä¸Šç–Šåˆé †åºåŠŸèƒ½BYå¥•ç¿”

import av
import os
import cv2
import time
import threading
from ultralytics import YOLO
import requests
import numpy as np
from skimage.metrics import structural_similarity as ssim
from collections import deque

# Force detection to use CPU
#device = "cpu"
#print(f"Using device: {device}")

import torch
print("torch =", torch.__version__, "cuda =", torch.version.cuda, "available =", torch.cuda.is_available())
import torchvision
print("torchvision =", torchvision.__version__)
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"
print(f"Using device: {DEVICE}")

# ============================================
#   LINEGPTï¼šToken å–å¾— + è¨Šæ¯/å½±åƒå‚³é€ï¼ˆä¼æ¥­ç´šï¼‰
# ============================================
# ===== å…¨åŸŸè®Šæ•¸ =====
_token_cache = {"token": None, "expire_time": 0}
_token_lock = threading.Lock()   # é˜²æ­¢å¤š Thread åŒæ™‚åˆ·æ–° Token

# ===== åŸºæœ¬è¨­å®š =====
LINE_LOGIN_URL = "https://lineapi.pcbut.com.tw:888/api/account/login"
LINE_NOTIFY_URL = "https://lineapi.pcbut.com.tw:888/api/notify-with-img"
LINE_USERNAME = "utbot"
LINE_PASSWORD = "mi2@admin5566"
DEFAULT_CHAT_ID = "2F0177B1-2AB0-471B-9001-E40B134F4D0F"   # æ¸¬è©¦ç”¨

# Alert settings
alert_threshold = 0.9  # Confidence threshold for alerts
alert_cooldown = 300  # Cooldown time in seconds
alert_records = []  # Store detection events for batch alerts
#last_alert_times = {}  # Store the last alert time for each camera
alert_interval = 300  # Batch alert interval (5 minutes)


# é¡åˆ¥å°æ‡‰çš„å†·å»æ™‚é–“
class_alert_cooldowns = {
        0: 300,  # ç«å…‰
        1: 300,  # ç…™éœ§
        2: 300,  # äººå“¡å€’è‡¥
        3: 300,  # æœªæˆ´æ‰‹å¥—
        4: 300,   # æœªæˆ´è­·ç›®é¡
        5: 300,   #ç´…è‰²æ¿å­
        6: 300,   #ç‰ˆé¢ä¸ä¸€è‡´
        7: 300,   #ç™½è‰²æ¿å­
        8: 300,   #ç–Šåˆç‰©æ–™æ•¸é‡ä¸æ­£ç¢º
        9: 300,
        10: 300,
        11: 300   #æ¨è»Š
    }

# Mutex lock to ensure thread safety
mutex = threading.Lock()

# Load YOLOv8 model (ensure this model is trained with the 7 specified classes)
# model_fire_smoke = YOLO('model/fire_smoke/best_0312.pt').to(0)  # Replace 'best.pt' with your model file
# model_fall = YOLO('model/fall/best_0318.pt').to(0)
# model_no_gloves = YOLO('model/gloves_goggles/best_0328.pt').to(0)
# #model_cellphone = YOLO('D:/AI/Demo_eden/edenTest/model_cellphone_label_studio_reduce_v2/model_collections/weights/best.pt').to(0)
# model_red_board = YOLO('model/red_board/best_0307.pt').to(0)


# Mapping from class index to event name (English)
class_event_mapping_en = {
    0: "fire",
    1: "smoke",
    2: "fall",
    3: "no gloves",
    4: "without goggles",
    5: "red board",
    6: "not_aligned",
    7: "white paper",
    8: "number_isnot_correct",
    9: "blue_gloves",
    10: "black_board",
    11: "cart"

}

# Mapping from class index to event name (Chinese)
class_event_mapping_cn = {
    0: "ç«å…‰",
    1: "ç…™éœ§",
    2: "äººå“¡å€’è‡¥",
    3: "æœªæˆ´æ‰‹å¥—",
    4: "æœªæˆ´è­·ç›®é¡",
    5: "ç´…è‰²æ¿æ–™",
    6: "ç‰ˆé¢ä¸ä¸€è‡´",
    7: "ç™½è‰²æ¿å­",
    8: "ç–Šåˆç‰©æ–™æ•¸é‡ä¸æ­£ç¢º",
    9: "è—è‰²æ‰‹å¥—",
    10: "é»‘è‰²æ¿å­",
    11: "æ¨è»Š"

}


color_dict ={
    0: (255,0,0), #ç´…
    1: (0,255,0), #ç¶ 
    2: (0,0,255), #è—
    3: (255,255,0), #æ·¡è—
    4: (255,0,255),
    5: (0,255,255),
    6: (255,255,255),
    7: (255,125,0),
    8: (125,125,125),
    9: (125,75,125),
    10: (75,125,75),
    11: (200,100,50) # æ¨è»Š - æ£•è‰²
}

last_alert_times = {}

camera_urls = {
                "501001":"rtsp://hikvision:Unitech0815!@10.80.21.130","501002":"rtsp://hikvision:Unitech0815!@10.80.21.131",
                "501005":"rtsp://hikvision:Unitech0815!@10.80.21.134","501006":"rtsp://hikvision:Unitech0815!@10.80.21.135",
                "501009":"rtsp://hikvision:Unitech0815!@10.80.21.138","501010":"rtsp://hikvision:Unitech0815!@10.80.21.139",
                "501013":"rtsp://hikvision:Unitech0815!@10.80.21.142","501014":"rtsp://hikvision:Unitech0815!@10.80.21.143"
                }

# camera_urls = {
#                 "501006":"C:/Users/vincent-shiu/Web/RecordFiles/2025-05-05/0505_v1_part3.mp4"
#                 }

model_specific_rois = {
    #æ‰‹å¥—è¾¨è­˜ç¯„åœï¼Œåªæœ‰501006é€™éš»æœƒé€²è¡Œé ç–Šåˆä½œæ¥­
    "501006": {
        "model_no_gloves": (50, 30, 1000, 680)
        },  
}

# 7æ–°å¢æ¨è»Šæª¢æ¸¬çš„æ’é™¤å€åŸŸè¨­å®š
cart_exclusion_rois = {
    "501005": (50, 30, 1000, 680)  # 501005æ”å½±æ©Ÿéœ€è¦æ’é™¤çš„å€åŸŸ
}

for i in camera_urls:
    if i not in last_alert_times:
        last_alert_times[i] = {}


# camera_rois = {"501006":(650,50,2050,1650)}
camera_rois = {
    "501006":{
                "white_paper_rois": [
                    (0, 90, 750, 1450),
                    (1950, 0, 3200, 1450),
                    (650, 50, 2050, 1650)

                ],
                "red_board_rois": [
                    (650, 50, 2050, 1650),
                    (1950, 0, 3200, 1450)
                ],
                "blue_gloves_rois":[
                    (650,25,2050,400),
                    (650,1000,2050,1600)
                ]
            } 
    }


####### ä¸­é–“ç™½è‰²ç´™å¼µè¨ˆæ™‚å™¨
# å®šç¾© ROI å€åŸŸ
ROI_COORDINATES = [(650, 50, 2050, 1650)]  # (x1, y1, x2, y2)
MIN_DETECTION_SECONDS = 3  # æœ€å°‘é€£çºŒåµæ¸¬æ™‚é–“ï¼ˆç§’ï¼‰

# åˆå§‹åŒ–è¨ˆæ™‚å™¨
roi_detection_times = {roi: 0 for roi in ROI_COORDINATES}
last_detection_times = {roi: 0 for roi in ROI_COORDINATES}

double_Flag = False


# æ¯æ”¯æ”å½±æ©Ÿçš„æœ€æ–°å¹€è¨˜æ†¶å€
frame_deques = {cam: deque(maxlen=1) for cam in camera_urls.keys()}

# æ¨¡å‹åˆå§‹åŒ–ï¼ˆGPU ID è¦–éœ€è¦èª¿æ•´ï¼‰
model_map = {
    "model_no_gloves": YOLO('model/gloves_goggles/best.pt').to(0),
    "model_fire_smoke": YOLO('model/fire_smoke/best.pt').to(0),
    "model_fall": YOLO('model/fall/best.pt').to(0),
    "model_red_board": YOLO('model/red_board/best.pt').to(0),
    "model_white_paper": YOLO('model/white_paper/best.pt').to(0),
    "model_stick_hand": YOLO('model/stick_hand/best.pt').to(0),
    "model_cart": YOLO('model/cart/best.pt').to(0)  # æ–°å¢æ¨è»Šæ¨¡å‹
}

# æ¯å€‹æ”å½±æ©Ÿåˆ†åˆ¥æ¨è«–å“ªäº›æ¨¡å‹
camera_models = { #"model_no_gloves", "model_fire_smoke","model_fall"
    "501001": [ "model_fire_smoke","model_fall"],
    "501005": [ "model_fire_smoke","model_fall","model_cart"],
    "501009": [ "model_fire_smoke","model_fall"],
    "501013": [ "model_fire_smoke","model_fall"],
    "501002": [ "model_red_board"],
    "501006": [ "model_no_gloves","model_red_board","model_white_paper"],
    "501010": [ "model_red_board"],
    "501014": [ "model_red_board"]
}


def receive_frames(cam_id, url):
    print(f"Starting receiver for {cam_id}")
    input_container = av.open(url, options={"hwaccel": "cuda"})

    for frame in input_container.decode(video=0):
        if stop_event.is_set():
            break
        img = frame.to_ndarray(format="bgr24")
        frame_deques[cam_id].append(img)
    input_container.close()
    print(f"Receiver stopped for {cam_id}")

def run_model(model, frame, detections,model_name,camera_index=None):

    if model_name == "model_red_board":
        results = model.predict(frame, conf=0.9,imgsz=640,verbose=False)
    else:
        # ä½¿ç”¨æ¨¡å‹é€²è¡Œæ¨è«–
        results = model.predict(frame, conf=0.9,imgsz=640,verbose=False)

    for result in results:
        for box in result.boxes:
            confidence = float(box.conf[0])
            cls = int(box.cls[0])  # Get class index
            
            x1_box, y1_box, x2_box, y2_box = map(int, box.xyxy[0])

            if model_name == "model_no_gloves":
                if confidence < alert_threshold:
                    continue
                cam_key = str(camera_index) if camera_index is not None else None
                x1_box, y1_box, x2_box, y2_box = map(int, box.xyxy[0])

                roi_gloves = None
                if cam_key and cam_key in model_specific_rois:
                    roi_gloves = model_specific_rois[cam_key].get("model_no_gloves")

                if roi_gloves is not None:
                    rx1, ry1, rx2, ry2 = roi_gloves
                    if (x1_box >= rx1) and (y1_box >= ry1) and (x2_box <= rx2) and (y2_box <= ry2):
                        adjusted_box = {
                            "model": model_name,  # æ·»åŠ æ¨¡å‹åç¨±
                            'xyxy': [x1_box, y1_box, x2_box, y2_box],
                            'conf': box.conf,
                            'cls': box.cls
                            }

                        detections.append(adjusted_box)
            
             # åœ¨ç¾æœ‰çš„ if model_name == "model_no_gloves": åˆ¤æ–·å¾Œæ–°å¢ï¼š
            elif model_name == "model_cart":
                if confidence < alert_threshold:
                    continue
                
                cam_key = str(camera_index) if camera_index is not None else None
                x1_box, y1_box, x2_box, y2_box = map(int, box.xyxy[0])
                
                # æ¨è»Šæ’é™¤å€åŸŸæª¢æŸ¥
                if cam_key == "501005":
                    exclude_roi = (50, 30, 1000, 680)  # æ’é™¤å€åŸŸåº§æ¨™
                    ex1, ey1, ex2, ey2 = exclude_roi
                    
                    # è¨ˆç®—æª¢æ¸¬æ¡†çš„ä¸­å¿ƒé»
                    center_x = (x1_box + x2_box) // 2
                    center_y = (y1_box + y2_box) // 2
                    
                    # å¦‚æœä¸­å¿ƒé»åœ¨æ’é™¤å€åŸŸå…§ï¼Œè·³éæ­¤æª¢æ¸¬
                    if ex1 <= center_x <= ex2 and ey1 <= center_y <= ey2:
                        continue
                
                # å¦‚æœä¸åœ¨æ’é™¤å€åŸŸå…§ï¼ŒåŠ å…¥æª¢æ¸¬çµæœ
                adjusted_box = {
                    "model": model_name,
                    'xyxy': [x1_box, y1_box, x2_box, y2_box],
                    'conf': box.conf,
                    'cls': box.cls
                }
                detections.append(adjusted_box)
            else:
                if confidence > alert_threshold:
                    # Get bounding box coordinates
                    x1_box, y1_box, x2_box, y2_box = map(int, box.xyxy[0])

                    adjusted_box = {
                            "model": model_name,  # æ·»åŠ æ¨¡å‹åç¨±
                            'xyxy': [x1_box, y1_box, x2_box, y2_box],
                            'conf': box.conf,
                            'cls': box.cls
                            }

                    detections.append(adjusted_box)

def run_model_with_cart_exclusion(model, frame, detections, model_name, camera_index=None):
    """
    ä¿®æ”¹ç‰ˆçš„ run_model å‡½æ•¸ï¼Œå°ˆé–€è™•ç†æ¨è»Šæ¨¡å‹çš„å€åŸŸæ’é™¤é‚è¼¯
    """
    if model_name == "model_cart":
        results = model.predict(frame, conf=0.9, imgsz=640, verbose=False)
    else:
        results = model.predict(frame, conf=0.9, imgsz=640, verbose=False)

    for result in results:
        for box in result.boxes:
            confidence = float(box.conf[0])
            cls = int(box.cls[0])
            
            x1_box, y1_box, x2_box, y2_box = map(int, box.xyxy[0])

            # æ¨è»Šæ¨¡å‹çš„ç‰¹æ®Šè™•ç† - æª¢æŸ¥æ˜¯å¦åœ¨æ’é™¤å€åŸŸå¤–
            if model_name == "model_cart":
                if confidence < alert_threshold:
                    continue
                
                cam_key = str(camera_index) if camera_index is not None else None
                
                # æª¢æŸ¥æ˜¯å¦æœ‰æ’é™¤å€åŸŸè¨­å®š
                if cam_key and cam_key in cart_exclusion_rois:
                    exclude_roi = cart_exclusion_rois[cam_key]
                    ex1, ey1, ex2, ey2 = exclude_roi
                    
                    # è¨ˆç®—æª¢æ¸¬æ¡†çš„ä¸­å¿ƒé»
                    center_x = (x1_box + x2_box) // 2
                    center_y = (y1_box + y2_box) // 2
                    
                    # å¦‚æœæª¢æ¸¬æ¡†çš„ä¸­å¿ƒé»åœ¨æ’é™¤å€åŸŸå…§ï¼Œå‰‡è·³éæ­¤æª¢æ¸¬
                    if ex1 <= center_x <= ex2 and ey1 <= center_y <= ey2:
                        continue
                
                # å¦‚æœä¸åœ¨æ’é™¤å€åŸŸå…§ï¼Œå‰‡åŠ å…¥æª¢æ¸¬çµæœ
                adjusted_box = {
                    "model": model_name,
                    'xyxy': [x1_box, y1_box, x2_box, y2_box],
                    'conf': box.conf,
                    'cls': box.cls
                }
                detections.append(adjusted_box)
            
            # å…¶ä»–æ¨¡å‹çš„åŸæœ‰é‚è¼¯ä¿æŒä¸è®Š
            else:
                if confidence > alert_threshold:
                    adjusted_box = {
                        "model": model_name,
                        'xyxy': [x1_box, y1_box, x2_box, y2_box],
                        'conf': box.conf,
                        'cls': box.cls
                    }
                    detections.append(adjusted_box)

def run_model_red_board(model, frame, detections,model_name,camera_index=None):
    #detections = []

    # if model_name == "model_stick_hand":
    #     results = model(frame, conf=0.3 ,verbose=False)
    # results = model(frame, conf=0.8 ,verbose=False)

    #results = model.predict(frame, conf=0.9,imgsz=640,verbose=False)
    red_board_detected = False
    if model_name == "model_red_board":
        results = model.predict(frame, conf=0.9,imgsz=640,verbose=False)
    
    for result in results:
        for box in result.boxes:
            confidence = float(box.conf[0])
            cls = int(box.cls[0])  # Get class index
            
            x1_box, y1_box, x2_box, y2_box = map(int, box.xyxy[0])
            
            if confidence > alert_threshold:
                red_board_detected = True

    return red_board_detected

def inference_worker(camera_index):
    models = camera_models.get(camera_index, [])

    red_board_count = 0 # çœ‹è¢«åˆ¤å®šnot alignedçš„åœ–ï¼Œä¸Šç·šå¾Œå¯çœç•¥æ­¤è®Šæ•¸
    red_board_frame = [] # é€£çºŒåµæ¸¬åˆ°red boardçš„buffer
    red_board_flag_first = True # ç´€éŒ„æ¯ä¸€æ‰¹æ¬¡çš„æ¨™æº–ç­”æ¡ˆ
    red_board_cooldown = 0 #  ç´€éŒ„ä¸­æ®µä½œæ¥­å€æ²’æœ‰æ–°åµæ¸¬çš„ç´…è‰²æ¿å­ç‰©ä»¶çš„æ™‚é–“
    red_board_ori_frame = []
    roi_frame_location_buffer = []

    standard_frame = None
    number_count_process = None
    red_board_start_time = None

    code_start_time = time.time()  # é‡æ–°è¨ˆæ™‚

    # åˆå§‹åŒ–æ‰‹å¥—ä½ç½®èˆ‡æ™‚é–“ç·©å­˜
    glove_positions = []  # æ‰‹å¥—åº§æ¨™æ¸…å–®
    glove_stable_time = []  # æ‰‹å¥—ç©©å®šæ™‚é–“
    stability_threshold = 5 # ç©©å®šæ™‚é–“é–¾å€¼ï¼ˆç§’ï¼‰
    position_tolerance = 100  # ä½ç½®èª¤å·®ï¼ˆåƒç´ ï¼‰
    distance_threshold = 400  # æ‰‹å¥—ä¹‹é–“è·é›¢é–¾å€¼ï¼ˆåƒç´ ï¼‰
    detection_success = False
    double_PP_start = False
    double_PP_end = time.time()
    red_board_right_time = 0
    double_PP_process_time= 0
    

    while not stop_event.is_set():
        if frame_deques[camera_index]:
            raw_frame = frame_deques[camera_index][-1]

            # ============================================================
            # ğŸ”¥ Frame å®‰å…¨æª¢æŸ¥ + é˜² Crash + æ¸…æ¥š LOGï¼ˆé€™ä¸€æ®µæ˜¯æ–°åŠ çš„ï¼‰
            # ============================================================
            if raw_frame is None:
                print(f"âŒ [Camera {camera_index}] æ”¶åˆ°ç„¡æ•ˆç•«é¢ï¼ˆraw_frame=Noneï¼‰ï¼Œè·³éæ­¤æ¬¡æ¨è«–")
                continue

            if not hasattr(raw_frame, "shape"):
                print(f"âŒ [Camera {camera_index}] raw_frame ç„¡ shapeï¼Œå¯èƒ½è§£ç¢¼å¤±æ•—ï¼Œè·³éæ­¤æ¬¡æ¨è«–")
                continue

            fh, fw = raw_frame.shape[:2]
            if fh == 0 or fw == 0:
                print(f"âŒ [Camera {camera_index}] raw_frame å°ºå¯¸ç•°å¸¸ï¼š{fw}x{fh}ï¼Œè·³éæ­¤æ¬¡æ¨è«–")
                continue

            # åœ¨é€™è£¡å…ˆè¨˜éŒ„å¥½ frame å¯¬é«˜ï¼Œå¾Œé¢æ‰€æœ‰ ROI é©—è­‰éƒ½å¯ä»¥å®‰å…¨ä½¿ç”¨
            frame_height = fh
            frame_width = fw
            # ============================================================
            # ğŸ”¥ æ–°å¢å€å¡ŠçµæŸï¼Œä¸‹é¢æ¥ä½ åŸæœ¬çš„ç¨‹å¼
            # ============================================================



            frame = raw_frame.copy()
            preview_frame = raw_frame.copy()
            detections = []

            red_board_flag = False #ç´€éŒ„æ¯æ¬¡çš„frameæœ‰æ²’æœ‰åµæ¸¬åˆ°red boardï¼Œé è¨­ç‚ºFalse
            white_paper_flag = False
            if frame is not None:
                if camera_index == "501005":
                    # ç¹ªè£½æ¨è»Šæ’é™¤å€åŸŸæ¡†ç·š
                    exclude_roi = (50, 30, 1000, 680)
                    ex1, ey1, ex2, ey2 = exclude_roi
                    
                    # é©—è­‰åº§æ¨™æœ‰æ•ˆæ€§
                    ex1 = max(0, min(ex1, frame_width - 1))
                    ex2 = max(0, min(ex2, frame_width))
                    ey1 = max(0, min(ey1, frame_height - 1)) 
                    ey2 = max(0, min(ey2, frame_height))
                    
                    if ex1 < ex2 and ey1 < ey2:
                        # ç¹ªè£½æ©˜è‰²æ¡†ç·š
                        cv2.rectangle(frame, (ex1, ey1), (ex2, ey2), (0, 165, 255), 5)
                        # åŠ ä¸Šæ¨™ç±¤
                        cv2.putText(frame, "CART EXCLUSION ZONE", (ex1, ey1 - 15),
                        cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 165, 255), 3)

                # æ‰“å°å½±åƒå°ºå¯¸ä»¥ç¢ºèª ROI æ˜¯å¦æœ‰æ•ˆ
                frame_height, frame_width = frame.shape[:2]
                # print(f"Camera {camera_index} - Frame size: {frame_width}x{frame_height}")
                
                # Retrieve ROI for this camera
                if str(camera_index) in camera_rois:
                    if "red_board_rois" in camera_rois[str(camera_index)]:

                        roi_frame = np.full_like(frame, (0, 0, 0))  #ç¶ è‰²èƒŒæ™¯
                        roi_red_board_frame_right = np.full_like(frame, (0, 0, 0))  #ç¶ è‰²èƒŒæ™¯

                        roi_sets = camera_rois[str(camera_index)]["red_board_rois"]
                        for roi in roi_sets:
                            x1, y1, x2, y2 = roi
                            # Validate ROI coordinates against frame size
                            x1 = max(0, min(x1, frame_width - 1))
                            x2 = max(0, min(x2, frame_width))
                            y1 = max(0, min(y1, frame_height - 1))
                            y2 = max(0, min(y2, frame_height))
                        

                            if x1 >= x2 or y1 >= y2:
                                print(f"Invalid white_paper_rois ROI for camera {camera_index}: {roi}. Skipping ROI processing.")
                                stop_event.set()
                                break
                                # roi = None
                                # roi_frame = frame
                            else:
                                # Draw ROI rectangle on the frame
                                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)

                                if x1 == 650:
                                    # å°‡å°æ‡‰ ROI å€å¡Šè¤‡è£½åˆ° roi_frameï¼ˆå…¶é¤˜ä»ç‚ºé»‘ï¼‰
                                    roi_frame[y1:y2, x1:x2] = preview_frame[y1:y2, x1:x2]
                                elif x1 == 1950:
                                    roi_red_board_frame_right[y1:y2, x1:x2] = preview_frame[y1:y2, x1:x2]


                    else:
                        print(f"{str(camera_index)} æ²’æœ‰ red_board_rois çš„ ROI...")
                        stop_event.set()
                        break

                    if "white_paper_rois" in camera_rois[str(camera_index)]:
                        # å»ºç«‹å…¨é»‘ç•«é¢
                        #roi_white_paper_frame = np.zeros_like(frame)
                        roi_white_paper_frame = np.full_like(frame, (0, 255, 0))  #ç¶ è‰²èƒŒæ™¯

                        roi_sets = camera_rois[str(camera_index)]["white_paper_rois"]
                        for roi in roi_sets:
                            x1, y1, x2, y2 = roi
                            # Validate ROI coordinates against frame size
                            x1 = max(0, min(x1, frame_width - 1))
                            x2 = max(0, min(x2, frame_width))
                            y1 = max(0, min(y1, frame_height - 1))
                            y2 = max(0, min(y2, frame_height))
                        

                            if x1 >= x2 or y1 >= y2:
                                print(f"Invalid white_paper_rois ROI for camera {camera_index}: {roi}. Skipping ROI processing.")
                                stop_event.set()
                                break
                                # roi = None
                                # roi_frame = frame
                            else:
                                # Draw ROI rectangle on the frame
                                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 10)
                                
                                # å°‡å°æ‡‰ ROI å€å¡Šè¤‡è£½åˆ° roi_frameï¼ˆå…¶é¤˜ä»ç‚ºé»‘ï¼‰
                                roi_white_paper_frame[y1:y2, x1:x2] = preview_frame[y1:y2, x1:x2]

                                # resized_roi_white_paper_frame = cv2.resize(roi_white_paper_frame, (640, 400))

                                # cv2.imshow(f"review_{camera_index}",resized_roi_white_paper_frame)

                                # if cv2.waitKey(1) & 0xFF == ord('q'):
                                #     stop_event.set()



                    else:
                        print(f"{str(camera_index)} æ²’æœ‰ white_paper_rois çš„ ROI...")
                        stop_event.set()
                        break
                    
                    if "blue_gloves_rois" in camera_rois[str(camera_index)]:
                        roi_blue_gloves_frame = np.full_like(frame, (0, 0, 0))  #ç¶ è‰²èƒŒæ™¯
                        roi_sets = camera_rois[str(camera_index)]["blue_gloves_rois"]
                        for roi in roi_sets:
                            x1, y1, x2, y2 = roi
                            # Validate ROI coordinates against frame size
                            x1 = max(0, min(x1, frame_width - 1))
                            x2 = max(0, min(x2, frame_width))
                            y1 = max(0, min(y1, frame_height - 1))
                            y2 = max(0, min(y2, frame_height))
                        

                            if x1 >= x2 or y1 >= y2:
                                print(f"Invalid blue_gloves_rois ROI for camera {camera_index}: {roi}. Skipping ROI processing.")
                                stop_event.set()
                                break
                                # roi = None
                                # roi_frame = frame
                            else:
                                # Draw ROI rectangle on the frame
                                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 10)
                                
                                # å°‡å°æ‡‰ ROI å€å¡Šè¤‡è£½åˆ° roi_frameï¼ˆå…¶é¤˜ä»ç‚ºé»‘ï¼‰
                                roi_blue_gloves_frame[y1:y2, x1:x2] = preview_frame[y1:y2, x1:x2]


                    else:
                        print(f"{str(camera_index)} æ²’æœ‰ blue_gloves çš„ ROI...")
                        stop_event.set()
                        break

                else:
                    roi = None
                    roi_frame = frame  # If no ROI defined, use the whole frame
                detections = []  # Store detections for alerts
                detections_v2 = []
                detected_gloves = [] #è—è‰²æ‰‹å¥—
                diff_Flag = False
                
                number_right_flag = False
                number_left_flag = False
                for m_name in models:

                    model = model_map[m_name]
                    
                    if m_name == "model_red_board":
                        if camera_index == "501006":
                            run_model(model, roi_frame, detections,m_name)
                            red_board_right_status = run_model_red_board(model, roi_red_board_frame_right, detections,m_name)
                            #detections.extend(run_model(model, roi_frame, detections,m_name))
                    elif m_name == "model_white_paper":
                        if camera_index == "501006":
                            run_model(model, roi_white_paper_frame, detections,m_name,camera_index)
                    elif m_name == "model_no_gloves":
                        if camera_index == "501006":
                            if red_board_flag_first == False:
                                run_model(model, preview_frame, detections,m_name)
                    elif m_name == "model_cart":
                        if camera_index == "501005":
                            run_model_with_cart_exclusion(model, preview_frame, detections, m_name, camera_index)
            
                    else:
                        run_model(model, preview_frame, detections,m_name)
                        #detections.extend(run_model(model, frame, detections,m_name))

                #wrong_time = time.strftime("%Y-%m-%d_%H-%M-%S")
                #white_paper_time = time.strftime("%Y-%m-%d_%H-%M-%S")
                for box in detections:

                    model_name = box["model"]

                    confidence = float(box['conf'][0])
                    cls = int(box['cls'][0])
                    
                    if model_name == "model_no_gloves":
                        if cls == 0:
                            #cv2.imwrite(f"original/{model_name}/{camera_index}_{wrong_time}_ori.jpg",preview_frame)
                            cls = 3
                            event_name_en = class_event_mapping_en.get(3, "Unknown Event")
                        elif cls == 1:
                            cls = 4
                            event_name_en = class_event_mapping_en.get(4, "Unknown Event")
              
                    if model_name == "model_fall":
                        #cv2.imwrite(f"original/{model_name}/{camera_index}_{wrong_time}_ori.jpg",preview_frame)

                        cls = 2
                        event_name_en = class_event_mapping_en.get(2, "Unknown Event")

                    if model_name == "model_cart":
                        cls = 11
                        event_name_en = class_event_mapping_en.get(11, "Unknown Event")    

                    if model_name == "model_fire_smoke":
                        #cv2.imwrite(f"original/{model_name}/{camera_index}_{wrong_time}_ori.jpg",preview_frame)
                        if cls == 0:
                            cls = 0
                            event_name_en = class_event_mapping_en.get(0, "Unknown Event")
                        else:
                            cls = 1
                            event_name_en = class_event_mapping_en.get(1, "Unknown Event")

                    if model_name == "model_red_board":

                        #red_board_detection = True
                        red_board_flag = True #æœ‰åµæ¸¬åˆ°ç´…è‰²æ¿å­
                        cls = 5
                        event_name_en = class_event_mapping_en.get(5, "Unknown Event")
                        red_board_frame.append(roi_frame) #æŠŠè¦æ¯”è¼ƒçš„åœ–å­˜åˆ°bufferè£¡é¢
                        red_board_ori_frame.append(preview_frame)
                        #print(len(red_board_frame))

                        x1_box, y1_box, x2_box, y2_box = box['xyxy']
                        roi_frame_location_buffer.append((x1_box, y1_box, x2_box, y2_box))

                        # print("red_board_frame :",len(red_board_frame))
                        
                        #ç•¶é€£çºŒåµæ¸¬åˆ°æ¿å­20å€‹frameå¾Œï¼Œä¸”ç¬¬20å€‹frameå®ƒæ˜¯æ­¤ä¸€æ‰¹æ¬¡ç¬¬ä¸€å€‹ç‰©ä»¶æˆ–æ˜¯å®ƒçš„ç´…è‰²æ¿å­å†·å»æ™‚é–“å·²åˆ°ï¼Œä»£è¡¨ä¸­æ®µå·²æŒçºŒè¶…écooldownçš„æ™‚é–“æ²’æœ‰åµæ¸¬åˆ°ç´…è‰²æ¿å­
                        # if (len(red_board_frame) == 20) and ((red_board_flag_first == True) or (red_board_cooldown == 0)):
                        if (len(red_board_frame) == 20) and (red_board_flag_first == True) :
                            #print("standard_frame :",standard_frame.shape)
                            x1_box, y1_box, x2_box, y2_box = box['xyxy']
                            standard_frame = preview_frame[y1_box:y2_box, x1_box:x2_box]

                            if standard_frame.shape[0] <= 1000 or standard_frame.shape[1] <=1000:
                                print("æ¨™æº–ç­”æ¡ˆçš„åœ–ç‰‡å°ºå¯¸æœ‰èª¤...")
                                break
                            standard_frame = standard_frame[300:600, 300:600]

                            # cv2.imwrite("test/ori.jpg",preview_frame)
                            # cv2.imwrite("test/sample.jpg",standard_frame)

                            #standard_frame = cv2.imread("test/sample.jpg")
                            red_board_flag_first = False

                            number_count_process = False

                            # detection_success = None

                        #ç•¶é€£çºŒåµæ¸¬åˆ°100å€‹frameså¾Œï¼Œé–‹å§‹æ¯”å°
                        # if (len(red_board_frame) == 60) and (red_board_flag_first == False):
                        #     # æ¯”å°ä¸‰å€‹å€åŸŸ
                        #     roi_frame = red_board_frame[-30]
                        #     ori_frame = red_board_ori_frame[-30]
                        if (len(red_board_frame) == 40) and (red_board_flag_first == False):
                            # æ¯”å°ä¸‰å€‹å€åŸŸ
                            roi_frame = red_board_frame[-20]
                            ori_frame = red_board_ori_frame[-20]

                            # x1_box, y1_box, x2_box, y2_box = box['xyxy']
                            x1_box, y1_box, x2_box, y2_box = roi_frame_location_buffer[-20]
                            roi_frame = roi_frame[y1_box:y2_box, x1_box:x2_box]

                            if roi_frame.shape[0] <= 1000 or roi_frame.shape[1] <= 1000:
                                print("è¢«æ¯”å°çš„åœ–ç‰‡å°ºå¯¸æœ‰èª¤...")
                                break

                            
                            roi_frame = roi_frame[300:600, 300:600]

                            gray1 = cv2.cvtColor(standard_frame, cv2.COLOR_BGR2GRAY)
                            gray2 = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2GRAY)

                            # gray1 = gray1[250:950, 300:850]
                            # gray2 = gray2[250:950, 300:850]

                            score = local_shifted_ssim(gray1, gray2)
                            #print(f"score : {score}")

                            if score < 0.2:
                                diff_Flag = True
                                #cv2.imwrite(f"test/not_aligned_{red_board_count}.jpg",roi_frame)

                            # if diff_Flag:

                            #     cv2.putText(frame, "Not Aligned", (50, 50),
                            #                 cv2.FONT_HERSHEY_SIMPLEX, 2, color_dict[cls], 10)

                            #     cv2.imwrite(f"test/not_aligned_{red_board_count}.jpg",roi_frame)

                            # red_board_frame.pop(0)
                            # red_board_ori_frame.pop(0)
                            red_board_frame = []
                            red_board_ori_frame = []
                            roi_frame_location_buffer = []
                        
                        # red_board_count = red_board_count + 1
                            
                    if model_name == "model_white_paper":
                        # cls 1 > white , cls 0 > black
                        detected = False
                        x1_box, y1_box, x2_box, y2_box = box['xyxy']

                        roi_sets = camera_rois[str(camera_index)]["white_paper_rois"]
                        for roi in roi_sets:
                            x1, y1, x2, y2 = roi
                            # Validate ROI coordinates against frame size
                            x1 = max(0, min(x1, frame_width - 1))
                            x2 = max(0, min(x2, frame_width))
                            y1 = max(0, min(y1, frame_height - 1))
                            y2 = max(0, min(y2, frame_height))
                        

                            if x1 >= x2 or y1 >= y2:
                                print(f"Invalid white_paper_rois ROI for camera {camera_index}: {roi}. Skipping ROI processing.")
                                stop_event.set()
                                break
                            else:
                                if not (x1 <= x1_box <= x2 and y1 <= y1_box <= y2 and x1 <= x2_box <= x2 and y1 <= y2_box <= y2):
                                    continue
                                detected = True
                                if x1 == 650:
                                    if not double_Flag:
                                        continue
                                    else:
                                        if cls == 1:
                                            white_paper_flag = True
                                            
                                            white_mid_time = time.time()
                                            if last_detection_times[roi] == 0:
                                                last_detection_times[roi] = white_mid_time
                                            
                                            roi_detection_times[roi] = white_mid_time - last_detection_times[roi]
                                            red_board_right_final_time = time.time() - red_board_right_time

                                            if (roi_detection_times[roi] >= MIN_DETECTION_SECONDS) and (red_board_right_final_time >= MIN_DETECTION_SECONDS):
                                                if standard_frame is None and double_PP_start == False and not red_board_right_status:
                                                    print(f"ç‰©ä»¶åœ¨ ROI {roi} é€£çºŒåµæ¸¬è¶…é {MIN_DETECTION_SECONDS} ç§’")
                                                double_PP_start = True



                                elif x1 == 0 or x1 == 1950:
                                    if cls == 1:
                                    
                                    
                                        if standard_frame is not None:
                                            x1_box, y1_box, x2_box, y2_box = box['xyxy']
                                            if x1_box < 1500:
                                                number_left_flag = True
                                            elif x1_box >= 1500:
                                                number_right_flag = True
                                    elif cls == 0:

                                        if standard_frame is not None:
                                            x1_box, y1_box, x2_box, y2_box = box['xyxy']
                                            if x1_box >= 1500:
                                                number_right_flag = True
                        
                        if detected:
                            if cls == 1:
                                cls = 7
                                event_name_en = class_event_mapping_en.get(7, "Unknown Event")
                            elif cls == 0:
                                cls = 10
                                event_name_en = class_event_mapping_en.get(10, "Unknown Event")
                        else:
                            continue



                        # if cls == 1:
                        #     cls = 7
                        #     event_name_en = class_event_mapping_en.get(7, "Unknown Event")
                        
                        
                        #     if standard_frame is not None:
                        #         x1_box, y1_box, x2_box, y2_box = box['xyxy']
                        #         if x1_box < 1500:
                        #             number_left_flag = True
                        #         elif x1_box >= 1500:
                        #             number_right_flag = True
                        # elif cls == 0:
                        #     cls = 10
                        #     event_name_en = class_event_mapping_en.get(10, "Unknown Event")
                        #     if standard_frame is not None:
                        #         x1_box, y1_box, x2_box, y2_box = box['xyxy']
                        #         if x1_box >= 1500:
                        #             number_right_flag = True
                    
            

                    if cls != 4:
                        # æ’é™¤æœªæˆ´çœ¼é¡
                        # ç•«bounding boxåœ¨preview frameä¸Š
                        x1_box, y1_box, x2_box, y2_box = box['xyxy']

                        cv2.rectangle(frame, (x1_box, y1_box), (x2_box, y2_box), color_dict[cls], 10)
                        label = f'{event_name_en} {confidence:.2f}'

                        cv2.putText(frame, label, (x1_box, y1_box - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 2, color_dict[cls], 10)
                    # try:
                    #     if cls == 7:
                    #         cv2.imwrite(f"result/{model_name}/{camera_index}_{white_paper_time}_result.jpg",frame)
                    # except Exception as e:
                    #     pass
                    
                    # try:
                    #     if cls == 3 or cls == 2 or cls == 0 or cls == 1:
                    #         cv2.imwrite(f"result/{model_name}/{camera_index}_{wrong_time}_result.jpg",frame)
                    # except Exception as e:
                    #     pass    

                if camera_index == "501006":
                    if double_Flag:
                        if not white_paper_flag:
                            roi_detection_times[roi] = 0
                            last_detection_times[roi] = 0

                        if red_board_right_status:
                            red_board_right_time = time.time()

                    ### ç•¶å€‹frameè‹¥ç™¼ç”Ÿç‰ˆé¢ä¸ä¸€è‡´ï¼Œå­˜çµæœ
                    if diff_Flag:
                            adjusted_box = {
                                            "model": "red_board_compare",  # æ·»åŠ æ¨¡å‹åç¨±
                                            'cls': 6,
                                            "detected_frame": ori_frame

                                            }
                            detections.append(adjusted_box)

                    ### ç–Šåˆç‰©æ–™æ˜¯å¦æ­£ç¢º        
                    if number_right_flag and number_left_flag:
                        if number_count_process is not None:
                            number_count_process = True
                    

                    #æ¯æ¬¡çš„frameéƒ½æœƒå»çœ‹é€™æ¬¡æœ‰æ²’æœ‰åµæ¸¬åˆ°red board
                    #å¦‚æœæ²’æœ‰å‰‡æ¸…ç©ºred board bufferï¼Œä¸”æœƒé–‹å§‹å€’æ•¸cooldownæ™‚é–“ï¼Œä¸”å¦‚æœcooldownå·²å€’æ•¸å®Œç•¢ï¼Œå‰‡æœƒæŠŠred_board_flag_firstè®Šæ•¸é‡æ–°reset
                    if not red_board_flag:
                        red_board_frame = []
                        red_board_ori_frame = []
                        roi_frame_location_buffer = []
                        # if red_board_cooldown > 0 :
                        #     red_board_cooldown = red_board_cooldown - 1

                        # åˆ¤æ–·æ˜¯å¦å·²ç¶“é–‹å§‹å€’æ•¸è¨ˆæ™‚
                        if red_board_start_time is None:
                            red_board_start_time = time.time()  # é–‹å§‹è¨ˆæ™‚ï¼ˆä»¥ç•¶å‰æ™‚é–“è¨˜éŒ„ï¼‰
                        # åˆ¤æ–·æ˜¯å¦è¶…é 1 åˆ†é˜
                        elapsed_time = time.time() - red_board_start_time
                        #elif red_board_cooldown == 0:

                        if elapsed_time >= 60:  # 60 ç§’
                            red_board_flag_first = True
                            # if os.path.exists("test/sample.jpg"):
                            #     os.remove("test/sample.jpg")
                            standard_frame = None

                            if number_count_process is not None:
                                if not number_count_process:
                                    process_time = time.strftime("%Y-%m-%d_%H-%M-%S")
                                    #print(f"ç–Šåˆç‰©æ–™æ•¸é‡ä¸æ­£ç¢º... , time is {process_time}")
                                    #cv2.imwrite(f"photo/{process_time}.jpg",preview_frame)
                                    adjusted_box = {
                                            "model": "number_count_incorrect",  # æ·»åŠ æ¨¡å‹åç¨±
                                            'cls': 8,
                                            "detected_frame": preview_frame

                                            }
                                    detections.append(adjusted_box)

                                #else:
                                    # æª¢æ¸¬æ•¸é‡æ­£ç¢ºï¼Œå°‡è®Šæ•¸resetè‡³None
                                    #print("ç–Šåˆç‰©æ–™æ•¸é‡æ­£ç¢º...")

                                number_count_process = None


                    else:
                        #å¦‚æœframeæœ‰åµæ¸¬åˆ°ç´…è‰²ç‰ˆå­ï¼Œå‰‡é‡æ–°å€’æ•¸
                        # red_board_cooldown = 3000
                        #red_board_cooldown = 500
                        red_board_start_time = time.time()  # é‡æ–°è¨ˆæ™‚


                if detections:
                    # å°‡è­¦å ±è™•ç†æ”¾å…¥ç¨ç«‹çš„ç·šç¨‹ï¼Œä»¥é¿å…é˜»å¡
                    alert_thread = threading.Thread(target=send_alert, args=(preview_frame.copy(), camera_index, detections), daemon=True)
                    alert_thread.start()

                    resized_frame = cv2.resize(frame, (640, 400))
                    cv2.imshow(f"{camera_index}",resized_frame)

                if cv2.waitKey(1) & 0xFF == ord('q'):
                    stop_event.set()

        else:
            time.sleep(0.01)

        # if camera_index == "UT5-1F-06":
        #     end_time = time.time()
        #     elapsed_time = end_time - start_time  # å–®ä½ç‚ºç§’
        #     print(f"è¿´åœˆåŸ·è¡Œæ™‚é–“ï¼š{elapsed_time:.4f} ç§’")


def local_shifted_ssim(img1, img2, window_size=(25, 25), stride=30, max_shift=5):
    """
    åœ¨æ»‘å‹•è¦–çª—æ™‚å®¹å¿å¾®å°ä½ç§» (max_shift)ï¼Œæ‰¾åˆ°æœ€ä½³å°é½Šå¾Œçš„ SSIMã€‚
    """
    img1 = cv2.resize(img1, (0, 0), fx=0.5, fy=0.5)
    img2 = cv2.resize(img2, (0, 0), fx=0.5, fy=0.5)

    h, w = img1.shape[:2]
    win_w, win_h = window_size
    ssim_scores = []

    for y in range(0, h - win_h, stride):

        for x in range(0, w - win_w, stride):
            
            roi1 = img1[y:y+win_h, x:x+win_w]

            max_score = -1
            # å°æ‡‰ç¯„åœå…§åšå¾®èª¿ (x_shift, y_shift)
            for dx in range(-max_shift, max_shift + 1):
                for dy in range(-max_shift, max_shift + 1):
                    xx = x + dx
                    yy = y + dy

                    if 0 <= xx < w - win_w and 0 <= yy < h - win_h:
                        roi2 = img2[yy:yy+win_h, xx:xx+win_w]
                        score = ssim(roi1, roi2)
                        max_score = max(max_score, score)
            
            if max_score != -1:
                ssim_scores.append(max_score)
    
    return np.mean(ssim_scores) if ssim_scores else 0


# =============================================================
# å–å¾— LINEGPT Tokenï¼ˆèåˆï¼šCache + Refresh + Thread-safeï¼‰
# =============================================================
def get_line_token(force_refresh=False):
    global _token_cache

    now = time.time()

    with _token_lock:

        # ---- Token å°šæœªéæœŸ â†’ ä½¿ç”¨ Cache ----
        if (not force_refresh
            and _token_cache["token"]
            and now < _token_cache["expire_time"]):
            return _token_cache["token"]

        # ---- Token éæœŸ â†’ é‡æ–°ç™»å…¥ ----
        try:
            resp = requests.post(
                LINE_LOGIN_URL,
                json={"username": LINE_USERNAME, "password": LINE_PASSWORD},
                verify=False,
                timeout=(8, 10)
            )

            if resp.status_code == 200:
                data = resp.json()
                token = data.get("token")

                if not token:
                    print("âŒ Token å›æ‡‰ç¼ºå°‘ token æ¬„ä½")
                    return None

                # æ›´æ–° Cache
                _token_cache["token"] = token
                _token_cache["expire_time"] = now + 3600 * 24 * 365

                print("âœ” Token å·²æ›´æ–°ï¼ˆCache ç”Ÿæ•ˆï¼‰")
                return token

            print(f"âŒ Login Token å¤±æ•—: {resp.status_code}")
            return None

        except Exception as e:
            print(f"âŒ Token API ä¾‹å¤–ï¼š{e}")
            return None


# =============================================================
# å‚³é€ LINEGPT è¨Šæ¯ï¼ˆå«åœ–ç‰‡ + Token Refreshï¼‰
# =============================================================
def send_line_message(message, file_path=None, retries=3):

    token = get_line_token()
    if not token:
        print("âŒ ç„¡æ³•å–å¾— Tokenï¼Œæ”¾æ£„å‚³é€")
        return False

    headers = {"Authorization": f"Bearer {token}"}
    data = {"message": message, "chatId": DEFAULT_CHAT_ID}

    for attempt in range(1, retries + 1):
        try:
            files = None
            if file_path and os.path.exists(file_path):
                files = {
                    "file": (
                        os.path.basename(file_path),
                        open(file_path, "rb"),
                        "image/jpeg"
                    )
                }

            resp = requests.post(
                LINE_NOTIFY_URL,
                headers=headers,
                data=data,
                files=files,
                verify=False,
                timeout=(10, 15)
            )

            # æˆåŠŸ
            if resp.status_code in (200, 201):
                print(f"âœ” LINEGPT å·²é€å‡ºï¼ˆattempt {attempt}ï¼‰")
                return True

            # Token éæœŸ â†’ åˆ·æ–°å¾Œé‡é€
            if resp.status_code == 401:
                print("âš  Token éæœŸ â†’ æ­£åœ¨å¼·åˆ¶åˆ·æ–°")
                token = get_line_token(force_refresh=True)
                headers = {"Authorization": f"Bearer {token}"}
                continue

            print(f"âŒ LINEGPT å›å‚³éŒ¯èª¤ï¼š{resp.status_code}")
            return False

        except Exception as e:
            print(f"âŒ LINEGPT ä¾‹å¤–ï¼š{e}")

        finally:
            if file_path and os.path.exists(file_path) and files:
                try:
                    files["file"].close()
                except:
                    pass

    return False


# =============================================================
# ç”¢ç”Ÿä¼æ¥­ç´šã€å½±åƒè¾¨è­˜é€šçŸ¥ã€‘æ ¼å¼
# =============================================================
def format_alert_message(location, feature, content):
    now_str = time.strftime("%Y-%m-%d %H:%M:%S")

    msg = f"""
ã€å½±åƒè¾¨è­˜é€šçŸ¥ã€‘
ç³»çµ±å·²åµæ¸¬åˆ°ç–‘ä¼¼é•è¦è¡Œç‚ºæˆ–æ½›åœ¨å®‰å…¨é¢¨éšªï¼š

ğŸ“ åœ°é»ï¼š{location}
ğŸ•’ æ™‚é–“ï¼š{now_str}
ğŸ§  ç‰¹å¾µé …ç›®ï¼š{feature}
ğŸ“„ å…§å®¹ï¼š{content}

è«‹å„˜é€Ÿè™•ç†æ­¤äº‹ä»¶ä¸¦ä¾æ“šå…¬å¸è¦å®šæ¡å–é©ç•¶è¡Œå‹•ã€‚
è‹¥éœ€è©³ç´°å½±åƒï¼Œè«‹è¯ç¹«è³‡è¨Šè™•ç³»çµ±ä¸€èª²ã€‚
å•é¡Œå›å ±ï¼šhttps://forms.gle/rFZXVRP1aUxqQNG97
""".strip()

    return msg


# =============================================================
# ğŸ”¥ send_alertï¼šæ•´åˆ Screenshot + LINEGPT + EIP
# =============================================================
def send_alert(send_frame, camera_index, detections):
    """
    ä¼æ¥­ç´šæ•´åˆï¼š
    - Screenshot
    - å†·å»æ§ç®¡
    - LINEGPT + åœ–ç‰‡
    - EIP UploadYoloImage
    """

    current_time = time.time()
    cls_buffer = []

    # ä¾ä½ çš„ class_alert_cooldowns / last_alert_times é‹è¡Œï¼ˆä¸æ”¹å‹•ä½ çš„é‚è¼¯ï¼‰
    for box in detections:
        model_name = box["model"]

        if model_name in ["model_red_board", "model_white_paper", "model_stick_hand"]:
            continue

        if model_name in ["red_board_compare", "number_count_incorrect"]:
            cls = int(box["cls"])
        else:
            cls = int(box["cls"])
            confidence = float(box["conf"][0])

        # é¡åˆ¥æ­¸ä¸€ï¼ˆä¾ä½ åŸæœ¬ 3/2/0/1/11ï¼‰
        if model_name == "model_no_gloves":
            cls = 3
        elif model_name == "model_fall":
            cls = 2
        elif model_name == "model_fire_smoke":
            cls = 0 if cls == 0 else 1
        elif model_name == "model_cart":
            cls = 11

        cooldown_time = class_alert_cooldowns.get(cls, 300)
        if cls not in last_alert_times[str(camera_index)]:
            last_alert_times[str(camera_index)][cls] = 0

        if current_time - last_alert_times[str(camera_index)][cls] <= cooldown_time:
            continue

        last_alert_times[str(camera_index)][cls] = current_time
        cls_buffer.append({**box, "cls": cls})

    if not cls_buffer:
        return

    # ========== Screenshot ==========
    timestamp = time.strftime("%Y-%m-%d_%H-%M-%S")
    filename = f"{camera_index}-{timestamp}.jpg"
    full_path = f"images/{filename}"

    for box in cls_buffer:
        x1, y1, x2, y2 = box['xyxy']
        cls = box["cls"]
        cv2.rectangle(send_frame, (x1, y1), (x2, y2), color_dict[cls], 10)

    success = cv2.imwrite(full_path, send_frame)
    if success:
        print(f"ğŸŸ© [Camera {camera_index}] Screenshot æˆåŠŸï¼š{filename} ({frame_width}x{frame_height})")
    else:
        print(f"ğŸŸ¥ [Camera {camera_index}] Screenshot å¤±æ•—ï¼š{filename}")
        print(f"    â¤ send_frame é¡å‹ï¼š{type(send_frame)}")
        try:
            print(f"    â¤ send_frame å°ºå¯¸ï¼š{send_frame.shape}")
        except:
            print("    â¤ send_frame ç„¡æ³•å–å¾— shapeï¼ˆå¯èƒ½ç‚º None æˆ– decode å¤±æ•—ï¼‰")

    # ========== äº‹ä»¶è³‡è¨Š ==========
    location = "åäºŒèª²é ç–Šåˆå®¤"
    event_names = [
        class_event_mapping_cn.get(int(box["cls"]), "æœªçŸ¥äº‹ä»¶")
        for box in cls_buffer
    ]
    formatted_event = "ï¼›".join(event_names)

    # ========== LINEGPT é€šçŸ¥ ==========
    try:
        alert_msg = format_alert_message(
            location=location,
            feature=formatted_event,
            content=f"æ”å½±æ©Ÿ {camera_index} åµæ¸¬åˆ°ï¼š{formatted_event}ï¼Œè«‹ç¾å ´ç¢ºèªã€‚"
        )

        send_line_message(
            alert_msg,
            file_path=full_path
        )

        print(f"âœ” LINEGPT å·²é€šçŸ¥ï¼š{filename}")

    except Exception as e:
        print(f"âŒ LINEGPT ç™¼é€ä¾‹å¤–ï¼š{e}")

    # ========== EIP ä¸Šå‚³ ==========
    try:
        api_url = "https://eip.pcbut.com.tw/File/UploadYoloImage"

        camera_model = {
            "cameraId": camera_index,
            "location": location,
            "eventName": formatted_event,
            "eventDate": time.strftime("%Y-%m-%d %H:%M:%S"),
            "notes": f"{len(cls_buffer)} events detected.",
            "fileName": filename,
            "result": f"ç–‘ä¼¼ç™¼ç”Ÿ {formatted_event}, è«‹åŒä»å„˜é€ŸæŸ¥çœ‹"
        }

        with open(full_path, "rb") as img_file:
            files = {"files": (filename, img_file, "image/jpeg")}
            r = requests.post(api_url, data=camera_model, files=files, verify=False)

        if r.status_code == 200:
            print(f"âœ” EIP ä¸Šå‚³æˆåŠŸï¼š{filename}")
        else:
            print(f"âŒ EIP å›å‚³ï¼š{r.status_code}, {r.text}")

    except Exception as e:
        print(f"âŒ EIP åŒæ­¥ä¾‹å¤–ï¼š{e}")


    #print(last_alert_times)

# Global stop event for all threads
stop_event = threading.Event()

def batch_alert():
    """
    è™•ç†æ‰¹æ¬¡è­¦å ±çš„å‡½æ•¸ï¼Œæ¯éš”ä¸€æ®µæ™‚é–“æª¢æŸ¥ä¸€æ¬¡ alert_records ä¸¦ç™¼é€å ±å‘Šã€‚
    """
    while not stop_event.is_set():
        time.sleep(alert_interval)
        with mutex:
            if alert_records:
                alert_message = f"Alert Report: {len(alert_records)} events detected."
                print(alert_message)
                # åœ¨æ­¤æ·»åŠ æ‰¹æ¬¡è­¦å ±éƒµä»¶ç™¼é€é‚è¼¯
                alert_records.clear()


if __name__ == '__main__':
    # Start batch alert processing thread #daemon: ä¸»ç¨‹å¼çµæŸæ™‚å¼·åˆ¶çµæŸthread
    alert_thread = threading.Thread(target=batch_alert, daemon=True)
    alert_thread.start()

    # Start threads for all cameras
    camera_threads = []
    # for index, url in enumerate(camera_urls):
    #     threads = process_camera(index, url)
    #     camera_threads.extend(threads)

    # global video
    # # è®€å–å½±ç‰‡
    # #cap = cv2.VideoCapture("D:/AI/Demo_eden/edenTest/datasets/fall/test_fall/queda.mp4")
    # cap = cv2.VideoCapture("demo_12_preLam_add_data.mp4")
    # output_video = "0303.mp4"
    # # ç¢ºä¿å½±ç‰‡æˆåŠŸè®€å–
    # if not cap.isOpened():
    #     print("ç„¡æ³•è®€å–å½±ç‰‡ï¼")
    #     exit()
    # # å–å¾—å½±ç‰‡è³‡è¨Š
    # fps = int(cap.get(cv2.CAP_PROP_FPS))  # å¹€ç‡
    # frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # å½±ç‰‡å¯¬åº¦
    # frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # å½±ç‰‡é«˜åº¦
    # # è¨­å®šå½±ç‰‡ç·¨ç¢¼æ ¼å¼èˆ‡è¼¸å‡ºç‰©ä»¶
    # fourcc = cv2.VideoWriter_fourcc(*"mp4v")  # å½±ç‰‡æ ¼å¼ï¼ˆmp4ï¼‰
    # video = cv2.VideoWriter(output_video, fourcc, fps, (frame_width, frame_height))


    # é–¾å€¼è¨­å®š
    similar_compare_threshold = 18  # å¯æ ¹æ“šå¯¦éš›æƒ…æ³èª¿æ•´

    for index, (key,value) in enumerate(camera_urls.items()):
        camera_threads.append(threading.Thread(target=receive_frames, args=(key, value), daemon=True))
        camera_threads.append(threading.Thread(target=inference_worker, args=(key,), daemon=True))
        # threads = process_camera(key, value)
        # camera_threads.extend(threads)
    for t in camera_threads:
        t.start()

    try:
        # Wait for all threads to complete
        while not stop_event.is_set():
            time.sleep(1)
    except KeyboardInterrupt:
        stop_event.set()
        print("Interrupt received, stopping all threads...")
    finally:
        for thread in camera_threads:
            thread.join(timeout=2)
        alert_thread.join(timeout=2)
        cv2.destroyAllWindows()
        print("All resources released, exiting.")